#!/usr/bin/env python3
import os
import argparse

from huggingface_hub import hf_hub_download, snapshot_download
from transformers import AutoTokenizer

from private_gpt.paths import models_path, models_cache_path
from private_gpt.settings.settings import settings

resume_download = True
skip_if_present = False
if __name__ == '__main__':
    parser = argparse.ArgumentParser(prog='Setup: Download models from huggingface')
    parser.add_argument(
        '--resume',
        default=True,
        action=argparse.BooleanOptionalAction,
        help='Enable/Disable resume_download options to restart the download progress interrupted'
    )
    parser.add_argument(
        '--skip',
        default=False, action=argparse.BooleanOptionalAction,
        help="Don't download if file(s) seem already downloaded. Handy for e.g. Docker."
    )
    args = parser.parse_args()
    resume_download = args.resume
    skip_if_present = args.skip

os.makedirs(models_path, exist_ok=True)

# Download Embedding model
embedding_path = models_path / "embedding"
print(f"Downloading embedding {settings().local.embedding_hf_model_name}")
snapshot_download(
    repo_id=settings().local.embedding_hf_model_name,
    cache_dir=models_cache_path,
    local_dir=embedding_path,
)
print("Embedding model downloaded!")

# Download LLM and create a symlink to the model file
print(f"Downloading LLM {settings().local.llm_hf_model_file}")
if skip_if_present:
    print("YOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO")
    print(settings().local.llm_hf_repo_id)
    print(settings().local.llm_hf_model_file)
    print(models_path)
    pass  # TODO check if model exists
else:
    hf_hub_download(
        repo_id=settings().local.llm_hf_repo_id,
        filename=settings().local.llm_hf_model_file,
        cache_dir=models_cache_path,
        local_dir=models_path,
        resume_download=resume_download,
    )
print("LLM model downloaded!")

# Download Tokenizer
print(f"Downloading tokenizer {settings().llm.tokenizer}")
AutoTokenizer.from_pretrained(
    pretrained_model_name_or_path=settings().llm.tokenizer,
    cache_dir=models_cache_path,
)
print("Tokenizer downloaded!")

print("Setup done")
