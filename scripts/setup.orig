#!/usr/bin/env python3
import os
import argparse
import urllib

from huggingface_hub import hf_hub_download, snapshot_download
from transformers import AutoTokenizer

from private_gpt.paths import models_path, models_cache_path
from private_gpt.settings.settings import settings

resume_download = True
ran_from_dockerfile = False
if __name__ == '__main__':
<<<<<<< HEAD
    parser = argparse.ArgumentParser(prog='Setup: Download models from huggingface')
    parser.add_argument(
        '--resume',
        default=True,
        action=argparse.BooleanOptionalAction,
        help='Enable/Disable resume_download options to restart the download progress interrupted'
    )
    parser.add_argument(
        '--docker',
        default=False,
        action=argparse.BooleanOptionalAction,
        help='Special mode for when setup is ran from dockerfile entrypoint '
             'that skips the setup in case of no Internet connection'
    )
=======
    parser = argparse.ArgumentParser(prog='Setup: Download models from Hugging Face')
    parser.add_argument('--resume', default=True, action=argparse.BooleanOptionalAction, help='Enable/Disable resume_download options to restart the download progress interrupted')
>>>>>>> main
    args = parser.parse_args()
    resume_download = args.resume
    ran_from_dockerfile = args.docker


def internet_connection():
    try:
        urllib.request.urlopen("https://www.google.com/")
        return True
    except urllib.error.URLError:
        return False


if not ran_from_dockerfile or (ran_from_dockerfile and internet_connection()):
    os.makedirs(models_path, exist_ok=True)

    # Download Embedding model, NOTE: doesn't re-download if ok
    embedding_path = models_path / "embedding"
    print(f"Downloading embedding {settings().huggingface.embedding_hf_model_name}")
    snapshot_download(
        repo_id=settings().huggingface.embedding_hf_model_name,
        cache_dir=models_cache_path,
        local_dir=embedding_path,
    )
    print("Embedding model downloaded!")

    # Download LLM and create a symlink to the model file, NOTE: doesn't re-download if ok
    print(f"Downloading LLM {settings().llamacpp.llm_hf_model_file}")
    hf_hub_download(
        repo_id=settings().llamacpp.llm_hf_repo_id,
        filename=settings().llamacpp.llm_hf_model_file,
        cache_dir=models_cache_path,
        local_dir=models_path,
        resume_download=resume_download,
    )
    print("LLM model downloaded!")

    # Download Tokenizer, NOTE: always re-download
    print(f"Downloading tokenizer {settings().llm.tokenizer}")
    AutoTokenizer.from_pretrained(
        pretrained_model_name_or_path=settings().llm.tokenizer,
        cache_dir=models_cache_path,
        # token="",  # token needed for e.g. llama2 tokenizer
    )
    print("Tokenizer downloaded!")

    print("Setup done")

else:  # ran from dockerfile but no Internet connection:
    # Don't want to break if there's no Internet <-> model might be setup correctly already
    print("NO INTERNET CONNECTION, ASSUMING MODEL IS SETUP CORRECTLY!")
