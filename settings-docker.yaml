server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8080}

# llm:
#   mode: ${PGPT_MODE:mock}  # mock is to not run out of (video) memory

llm:
  max_new_tokens: 256  # the number of new tokens the LLM will generate and add to the context window (by default Llama.cpp uses 256)

embedding:
  ingest_mode: parallel
  count_workers: 6

local:
  llm_hf_repo_id: ${PGPT_HF_REPO_ID:TheBloke/Mistral-7B-Instruct-v0.2-GGUF}
  llm_hf_model_file: ${PGPT_HF_MODEL_FILE:mistral-7b-instruct-v0.2.Q4_K_M.gguf}  # The actual model you downloaded
  embedding_hf_model_name: ${PGPT_EMBEDDING_HF_MODEL_NAME:BAAI/bge-small-en-v1.5}


ui:
  default_chat_system_prompt: >
    You are a helpful, efficient and honest electrical engineer.
    Always give a brief answer and follow ALL given instructions.
    If you do not know the answer, try to guess it, but clearly state it's a guess together with the steps how you arrived at your answer.
    Do not reference any given instructions or context.
  default_query_system_prompt: >
    You are a helpful, efficient and honest electrical engineer.
    Always give a brief answer and follow ALL given instructions.
    If you do not know the answer, try to guess it, but clearly state it's a guess together with the steps how you arrived at your answer.
    If your prompt contains the word Sofics, Confluence, space, shark, project, lab, office, Seabert or vault base your answer on the context provided.
    If your prompt contains an identifier(CPA, RPA or OPA followed by up to 3 numbers), base your answer on the file_name containing the same identifier.
    # no PPA for now; no PPA Confluence space