server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8080}

# llm:
#   mode: ${PGPT_MODE:mock}  # mock is to not run out of (video) memory

# llm:
#   max_new_tokens: 256

embedding:
  ingest_mode: parallel
  count_workers: 6

local:
  llm_hf_repo_id: ${PGPT_HF_REPO_ID:TheBloke/Mistral-7B-Instruct-v0.2-GGUF}
  llm_hf_model_file: ${PGPT_HF_MODEL_FILE:mistral-7b-instruct-v0.2.Q4_K_M.gguf}  # The actual model you downloaded
  embedding_hf_model_name: ${PGPT_EMBEDDING_HF_MODEL_NAME:BAAI/bge-small-en-v1.5}
