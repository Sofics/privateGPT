server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8080}

# llm:
#   mode: ${PGPT_MODE:mock}  # mock is to not run out of (video) memory

llm:
  max_new_tokens: 256  # the max number of new tokens the LLM will generate and add to the context window (by default Llama.cpp uses 256)
  context_window: 32768  # NOTE: 32768 is too big for CPU without context filter, in that case use 3900 - 12000
  tokenizer: mistralai/Mistral-7B-Instruct-v0.2  # original HF model where tokenizer.json lives

embedding:
  ingest_mode: parallel
  count_workers: 6

local:
  llm_hf_repo_id: ${PGPT_HF_REPO_ID:TheBloke/Mistral-7B-Instruct-v0.2-GGUF}
  llm_hf_model_file: ${PGPT_HF_MODEL_FILE:mistral-7b-instruct-v0.2.Q4_K_M.gguf}  # The actual model you downloaded
  # Note: Besides having quality data, choosing a good embedding model is the most important and underrated step for optimizing your RAG application
  # HF leaderboard (up-to-date?): https://huggingface.co/spaces/mteb/leaderboard:
  # WhereIsAI/UAE-Large-V1
  # BAAI/bge-large-en-v1.5
  # BAAI/bge-base-en-v1.5
  # TODO changing embedding model => "could not broadcast input array from shape"? => wipe local_data folder
  embedding_hf_model_name: ${PGPT_EMBEDDING_HF_MODEL_NAME:BAAI/bge-base-en-v1.5}  # originally BAAI/bge-small-en-v1.5


ui:
  default_chat_system_prompt: >
    You are a helpful and honest electrical engineer specialized in solutions for integrated circuits.
    Always give a concise answer. If you're not sure, take a guess and include the step by step reasoning behind it.
  # TODO mention focusing on context if possible?
  default_query_system_prompt: >
    You are a helpful and honest electrical engineer specialized in solutions for integrated circuits.
    Always give a concise answer. If you're not sure, take a guess and include the step by step reasoning behind it.
    
