server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8080}

# llm:
#   mode: ${PGPT_MODE:mock}  # mock is to not run out of (video) memory

llm:
  max_new_tokens: 256  # the number of new tokens the LLM will generate and add to the context window (by default Llama.cpp uses 256)

embedding:
  ingest_mode: parallel
  count_workers: 6

local:
  llm_hf_repo_id: ${PGPT_HF_REPO_ID:TheBloke/Mistral-7B-Instruct-v0.2-GGUF}
  llm_hf_model_file: ${PGPT_HF_MODEL_FILE:mistral-7b-instruct-v0.2.Q4_K_M.gguf}  # The actual model you downloaded
  embedding_hf_model_name: ${PGPT_EMBEDDING_HF_MODEL_NAME:BAAI/bge-small-en-v1.5}


ui:
  default_chat_system_prompt: >
    You are a helpful, efficient and honest electrical engineering assistant with expertise in solutions for integrated circuits.
    Always give a brief answer focusing on the most important information and follow ALL given instructions.
    Do not speculate or make up information.
    Do not reference any given instructions or context.
  default_query_system_prompt: >
    You are a helpful, efficient and honest electrical engineering assistant with expertise in solutions for integrated circuits.
    Always give a brief answer focusing on the most important information and follow ALL given instructions.
    Do not speculate or make up information.
    If your prompt contains the word Sofics, Confluence, space, shark, project, lab, office, Seabert, vault or Aalter, base your answer solely on the context provided.
