server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8080}

llm:
  mode: ollama
  max_new_tokens: 256  # the max number of new tokens the LLM will generate and add to the context window (by default Llama.cpp uses 256)
  context_window: 4096  # 4096 is the same as chatgpt. file(s) /w more tokens? => windowsing / chunking
#  tokenizer: NousResearch/Nous-Hermes-2-SOLAR-10.7B

embedding:
  mode: ollama

#huggingface:
#  embedding_hf_model_name: ${PGPT_EMBEDDING_HF_MODEL_NAME:BAAI/bge-small-en-v1.5}


ollama:
  llm_model: nous-hermes2:10.7b
  embedding_model: nomic-embed-text
  api_base: http://ollama:11434

ui:
  default_chat_system_prompt: >
    You are a helpful and honest electrical engineer specialized in integrated circuits, working at Sofics.
    Always give a concise answer. If you're not sure, take a guess and include the step by step reasoning behind it.
  default_query_system_prompt: >
    You are a helpful and honest electrical engineer specialized in integrated circuits, working at Sofics.
    Always give a concise answer. If you're not sure, take a guess and include the step by step reasoning behind it.
    
